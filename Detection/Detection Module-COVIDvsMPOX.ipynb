{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d84d91",
   "metadata": {},
   "source": [
    "# Detection Module\n",
    "\n",
    "    The main goal of the detection module is to use the gazetteers out of the ontologies used to enrich PropaPhen into PropaPhen+ to discover relationships between network nodes/systems and the gufo:Entities by text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "110efc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ceac9",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50975ef2",
   "metadata": {},
   "source": [
    "### Installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d1a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install tqdm\n",
    "#!pip install nltk\n",
    "#!pip install gatenlp\n",
    "#!pip install py4j\n",
    "#!pip install pyodide\n",
    "#!pip install ipywidgets\n",
    "#!pip install neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d43728",
   "metadata": {},
   "source": [
    "### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f5246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbaeaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gatenlp import Document\n",
    "from gatenlp.gateworker import GateWorker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438eb90",
   "metadata": {},
   "source": [
    "### Custom libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6729226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('lib/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7296f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection.relationshipextraction import RelationshipDiscovery, GateExtractor, CleanDicts, rmToRelationCSV\n",
    "from detection.schema import Term, Concept, df_to_concepts, cleaningPlaceStr, conceptsToGazetteer\n",
    "from detection.worldumls import umlsConceptCleanner, isEnglish, worldConceptCleanner\n",
    "from detection.worldumls import ClearnWorldKGGazetteer\n",
    "#import detection.observationclustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa39dd",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55a0d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_covid_journals = \"data/textual/covid/newspaper/\"\n",
    "path_to_kb_gazetteer = '../data/gazetteers/kbgazetteer.csv'\n",
    "path_to_netwoork_gazetteer = '../data/gazetteers/world_gazetteer_en.csv'\n",
    "path_to_lsts = \"data/lst/\"\n",
    "path_to_relation_folder = \"../data/neo4j/\"\n",
    "path_to_covid_journalobservationcsv = \"../data/neo4j/covid_observations_journal.csv\"\n",
    "path_to_covid_medicalobservationcsv = \"../data/neo4j/covid_observations_medical.csv\"\n",
    "path_to_covid_socialobservationcsv = \"../data/neo4j/covid_observations_social.csv\"\n",
    "path_to_monkeypox_journalobservationcsv = \"../data/neo4j/monkeypox_observations_journal.csv\"\n",
    "path_to_monkeypox_medicalobservationcsv = \"../data/neo4j/monkeypox_observations_medical.csv\"\n",
    "path_to_monkeypox_socialobservationcsv = \"../data/neo4j/monkeypox_observations_social.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f1ea4",
   "metadata": {},
   "source": [
    "## Relationship Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e82cd",
   "metadata": {},
   "source": [
    "### KB Gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "560db393",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_concept_list = []\n",
    "network_concept_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d665080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kb = pd.read_csv(path_to_kb_gazetteer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efb8f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C0026106</td>\n",
       "      <td>Mild mental retardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C0026351</td>\n",
       "      <td>Moderate mental retardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C0036857</td>\n",
       "      <td>Severe mental retardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C0020796</td>\n",
       "      <td>Profound mental retardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C0025362</td>\n",
       "      <td>Unspecified mental retardation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        ID                            Name\n",
       "0           0  C0026106         Mild mental retardation\n",
       "1           1  C0026351     Moderate mental retardation\n",
       "2           2  C0036857       Severe mental retardation\n",
       "3           3  C0020796     Profound mental retardation\n",
       "4           4  C0025362  Unspecified mental retardation"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c71050a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12620098it [13:09, 15993.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Term list\n"
     ]
    }
   ],
   "source": [
    "kb_concept_list = df_to_concepts(df_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f22d91e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 7892473/7892473 [00:35<00:00, 220861.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(kb_concept_list))):\n",
    "    kb_concept_list[i] = umlsConceptCleanner(kb_concept_list[i])\n",
    "    kb_concept_list[i] = umlsConceptCleanner(kb_concept_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41aba865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 7892473/7892473 [02:19<00:00, 56774.96it/s]\n"
     ]
    }
   ],
   "source": [
    "umlsdict = conceptsToGazetteer(kb_concept_list,path_to_lsts+\"umls.lst\",cleaningPlaceStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21f141",
   "metadata": {},
   "source": [
    "### Place Gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db6bdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network = pd.read_csv(path_to_netwoork_gazetteer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "834eefba",
   "metadata": {},
   "outputs": [],
   "source": [
    "washingtonRemoveDoubles = ('wkg:158368533', \"Washington\")\n",
    "bradFord = (\"wkg:26701367\",\"Bradford\")\n",
    "def removeDoublesInNet(df_network,tupleList):\n",
    "    list_id_to_remove = []\n",
    "    for index, row in df_network.iterrows():\n",
    "        for tupleRemoveDoubles in tupleList:\n",
    "            if tupleRemoveDoubles[1] in row['Name'] and row['ID']!= tupleRemoveDoubles[0]:\n",
    "                list_id_to_remove.append(row['ID'])\n",
    "\n",
    "    df_network = df_network.drop(df_network[df_network.ID.isin(list_id_to_remove)].index.tolist())\n",
    "    return df_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66929659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network = removeDoublesInNet(df_network, [washingtonRemoveDoubles,bradFord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc09a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_net_list = ['\"Nga\"', '\"Centre\"', '\"Kou\"', '\"San\"','\"Real\"',\n",
    "                 '\"Vincent\"', '\"Lille\"','\"North\"', '\"Barr\"', '\"North\"'\n",
    "                 ,'\"South\"','\"West\"','\"East\"','\"Brito\"', '\"Utrecht\"', '\"Bush\"',\n",
    "                 '\"Bush\"', '\"Republic\"','\"Union\"', '\"Time\"',\n",
    "                 '\"Institute\"','\"Carbon\"','\"Center\"','\"Delhi\"','\"Mendenhall\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26be0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network = ClearnWorldKGGazetteer(df_network,clear_net_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d60d0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wkg:10</td>\n",
       "      <td>\"Mamassita\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wkg:10</td>\n",
       "      <td>\"Mamacita\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wkg:1000709658</td>\n",
       "      <td>\"Boulzazen\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>wkg:1000709658</td>\n",
       "      <td>\"Boulzazen\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wkg:1000709660</td>\n",
       "      <td>\"Tizi El Oued\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              ID            Name\n",
       "0           0          wkg:10     \"Mamassita\"\n",
       "1           1          wkg:10      \"Mamacita\"\n",
       "2           2  wkg:1000709658     \"Boulzazen\"\n",
       "3           3  wkg:1000709658     \"Boulzazen\"\n",
       "4           4  wkg:1000709660  \"Tizi El Oued\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_network.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40e0a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1692247it [02:02, 13792.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Term list\n"
     ]
    }
   ],
   "source": [
    "network_concept_list = df_to_concepts(df_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e35554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing network\n",
    "#for i in tqdm(range(len(network_concept_list))):\n",
    "#    network_concept_list[i] = worldConceptCleanner(network_concept_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63ccbe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usual name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 948962/948962 [00:02<00:00, 357207.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Normal\n",
    "print(\"Usual name\")\n",
    "normalplacesdict = conceptsToGazetteer(network_concept_list,path_to_lsts+\"places.lst\",cleaningPlaceStr)\n",
    "# Cap\n",
    "#print(\"Cap name\")\n",
    "#capdict = conceptsToGazetteer(network_concept_list,path_to_lsts+\"places_cap.lst\",capPlaceStr)\n",
    "# Lower\n",
    "#print(\"Lower name\")\n",
    "#lowerdict = conceptsToGazetteer(network_concept_list,path_to_lsts+\"places_lower.lst\",lowerPlaceStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd1b0e",
   "metadata": {},
   "source": [
    "### GATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ade2519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs = GateWorker(start=False, auth_token=\"1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2c7035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def cleanKeys(dictionary, clean_list):\n",
    "    for c in clean_list:\n",
    "        if c in dictionary:\n",
    "            del dictionary[c]\n",
    "    return dictionary\n",
    "\n",
    "def CleanDicts(netdict,kbdict):\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    punctuation = [i for i in string.punctuation  ]\n",
    "    stopwords_list_maj = [s.title() for s in stopwords_list]\n",
    "    months = [\"January\", \"February\", \"March\", \"April\", \"May\",\n",
    "              \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "    months_lower = [m.lower() for m in months]\n",
    "    clean_list = stopwords_list + punctuation + list(\n",
    "        string.ascii_lowercase) + list(\n",
    "        string.ascii_uppercase) + stopwords_list_maj + months + months_lower\n",
    "    netdict = cleanKeys(netdict,clean_list) \n",
    "    kbdict = cleanKeys(kbdict,clean_list+list(netdict.keys()))\n",
    "    return netdict, kbdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a7c893e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "normalplacesdict, umlsdict = CleanDicts(normalplacesdict, umlsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b82c956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KB gazetteer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 7657560/7657560 [17:09<00:00, 7436.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Network gazetteer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1217356/1217356 [01:11<00:00, 16960.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Merging gazetteer...\n"
     ]
    }
   ],
   "source": [
    "gateExtractor = GateExtractor(umlsdict,normalplacesdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c29edd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANNIE'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Annie\n",
    "gs.worker.loadMavenPlugin(\"uk.ac.gate.plugins\", \"annie\", \"8.6\")\n",
    "# now load the prepared ANNIE pipeline from the plugin\n",
    "pipeline = gs.worker.loadPipelineFromPlugin(\"uk.ac.gate.plugins\",\"annie\", \"/resources/ANNIE_with_defaults.gapp\")\n",
    "pipeline.getName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e739724",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateExtractor.extra_pr['annie'] = pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65c2a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection.relationshipextraction import RelationMatrix\n",
    "\n",
    "class RMGenerator():\n",
    "    \n",
    "    def __init__(self, corpus,gateExtractor, gs):\n",
    "        self.corpus = corpus\n",
    "        self.gateExtractor = gateExtractor\n",
    "        self.gs = gs\n",
    "    \n",
    "    def directTermMatching(self, matrix_id):\n",
    "        rm = RelationMatrix(matrix_id)\n",
    "        # Per document\n",
    "        for doc in tqdm(self.corpus):\n",
    "            pdoc = self.gs.gdoc2pdoc(doc)\n",
    "            pdoc = self.gateExtractor.tokenizer(pdoc)\n",
    "            pdoc = self.gateExtractor.tok_gaz(pdoc)\n",
    "            # Making the rm links\n",
    "            for kb_annotation in pdoc.annset().with_type(\"kb\"):\n",
    "                for network_annoation in pdoc.annset().with_type(\"network\"):\n",
    "                    rm.increaseBy(self.gateExtractor.dict_kb[kb_annotation.features['key']], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "            self.gs.del_resource(doc)\n",
    "        return rm\n",
    "    \n",
    "    def paragraphTermMatching(self, matrix_id):\n",
    "        assert 'annie' in self.gateExtractor.extra_pr.keys()\n",
    "        rm = RelationMatrix(matrix_id)\n",
    "        # Per document\n",
    "        for doc in tqdm(self.corpus):\n",
    "            # Run annie\n",
    "            if len(self.gs.gdoc2pdoc(doc).text) <= 0:\n",
    "                self.gs.del_resource(doc)\n",
    "                continue\n",
    "            self.gs.worker.run4Document(self.gateExtractor.extra_pr['annie'], doc)\n",
    "            pdoc = self.gs.gdoc2pdoc(doc)            \n",
    "            # Get network and kb\n",
    "            pdoc = self.gateExtractor.tok_gaz(pdoc)\n",
    "            # Get paragraph\n",
    "            praragraphann = pdoc.annset('Original markups').with_type(\"paragraph\")\n",
    "            # For each paragraph\n",
    "            for ann in praragraphann:\n",
    "                # Making the rm links\n",
    "                for kb_annotation in pdoc.annset().within(ann).with_type('kb'):\n",
    "                    for network_annoation in pdoc.annset().within(ann).with_type('network'):\n",
    "                        rm.increaseBy(self.gateExtractor.dict_kb[kb_annotation.features['key']], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "            self.gs.del_resource(doc)\n",
    "        return rm\n",
    "    \n",
    "    def paragraphTermMatchingTransitivity(self, matrix_id):\n",
    "        assert 'annie' in self.gateExtractor.extra_pr.keys()\n",
    "        rm = RelationMatrix(matrix_id)\n",
    "        # Per document\n",
    "        for doc in tqdm(self.corpus):\n",
    "            # Run annie\n",
    "            if len(self.gs.gdoc2pdoc(doc).text) <= 0:\n",
    "                self.gs.del_resource(doc)\n",
    "                continue\n",
    "            self.gs.worker.run4Document(self.gateExtractor.extra_pr['annie'], doc)\n",
    "            pdoc = self.gs.gdoc2pdoc(doc)            \n",
    "            # Get network and kb\n",
    "            pdoc = self.gateExtractor.tok_gaz(pdoc)\n",
    "            # Get paragraph\n",
    "            paragraphann = pdoc.annset('Original markups').with_type(\"paragraph\")\n",
    "            dictKbToKb = {}\n",
    "            # For each paragraph\n",
    "            for ann in paragraphann:\n",
    "                # Making Refs between KB entities\n",
    "                for kb_annotation1 in pdoc.annset().within(ann).with_type('kb'):\n",
    "                    for kb_annotation2 in pdoc.annset().within(ann).with_type('kb'):\n",
    "                        # if same annotation continue\n",
    "                        if kb_annotation1 == kb_annotation2:\n",
    "                            continue\n",
    "                        # If empty list create list\n",
    "                        if kb_annotation1.features['key'] not in dictKbToKb:\n",
    "                                dictKbToKb[kb_annotation1.features['key']] = []\n",
    "                        # Add key to list\n",
    "                        dictKbToKb[kb_annotation1.features['key']] = dictKbToKb[\n",
    "                            kb_annotation1.features['key']] +  [kb_annotation2.features['key']]\n",
    "            for ann in paragraphann:\n",
    "                # Making the rm links\n",
    "                for kb_annotation in pdoc.annset().within(ann).with_type('kb'):\n",
    "                    for network_annoation in pdoc.annset().within(ann).with_type('network'):\n",
    "                        rm.increaseBy(self.gateExtractor.dict_kb[kb_annotation.features['key']], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "            # Adding transitivity relations\n",
    "            for ann in paragraphann:\n",
    "                # Making the rm links\n",
    "                for network_annoation in pdoc.annset().within(ann).with_type('network'):\n",
    "                    for kb_annotation in pdoc.annset().within(ann).with_type('kb'):\n",
    "                        if kb_annotation.features['key'] not in dictKbToKb:\n",
    "                            continue\n",
    "                        for transitivityKey in dictKbToKb[kb_annotation.features['key']]:\n",
    "                            if rm.getValue(self.gateExtractor.dict_kb[transitivityKey], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']]) is None:\n",
    "                                # If link does not exists, then create one\n",
    "                                rm.increaseBy(self.gateExtractor.dict_kb[transitivityKey], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "                            \n",
    "            self.gs.del_resource(doc)\n",
    "        return rm\n",
    "    \n",
    "    def sentenceTermMatching(self, matrix_id):\n",
    "        assert 'annie' in self.gateExtractor.extra_pr.keys()\n",
    "        rm = RelationMatrix(matrix_id)\n",
    "        # Per document\n",
    "        for doc in tqdm(self.corpus):\n",
    "            # Run annie\n",
    "            if len(self.gs.gdoc2pdoc(doc).text) <= 0:\n",
    "                self.gs.del_resource(doc)\n",
    "                continue\n",
    "            self.gs.worker.run4Document(self.gateExtractor.extra_pr['annie'], doc)\n",
    "            pdoc = self.gs.gdoc2pdoc(doc)            \n",
    "            # Get network and kb\n",
    "            pdoc = self.gateExtractor.tok_gaz(pdoc)\n",
    "            # Get paragraph\n",
    "            sentenceann = pdoc.annset('').with_type(\"Sentence\")\n",
    "            # For each paragraph\n",
    "            for ann in sentenceann:\n",
    "                # Making the rm links\n",
    "                for kb_annotation in pdoc.annset().within(ann).with_type('kb'):\n",
    "                    for network_annoation in pdoc.annset().within(ann).with_type('network'):\n",
    "                        rm.increaseBy(self.gateExtractor.dict_kb[kb_annotation.features['key']], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "            self.gs.del_resource(doc)\n",
    "        return rm\n",
    "    \n",
    "    def sentenceTermMatchingTransitivity(self, matrix_id):\n",
    "        assert 'annie' in self.gateExtractor.extra_pr.keys()\n",
    "        rm = RelationMatrix(matrix_id)\n",
    "        # Per document\n",
    "        for doc in tqdm(self.corpus):\n",
    "            # Run annie\n",
    "            if len(self.gs.gdoc2pdoc(doc).text) <= 0:\n",
    "                self.gs.del_resource(doc)\n",
    "                continue\n",
    "            self.gs.worker.run4Document(self.gateExtractor.extra_pr['annie'], doc)\n",
    "            pdoc = self.gs.gdoc2pdoc(doc)            \n",
    "            # Get network and kb\n",
    "            pdoc = self.gateExtractor.tok_gaz(pdoc)\n",
    "            # Get paragraph\n",
    "            sentenceann = pdoc.annset('').with_type(\"Sentence\")\n",
    "            dictKbToKb = {}\n",
    "            # For each paragraph\n",
    "            for ann in sentenceann:\n",
    "                # Making Refs between KB entities\n",
    "                for kb_annotation1 in pdoc.annset().within(ann).with_type('kb'):\n",
    "                    for kb_annotation2 in pdoc.annset().within(ann).with_type('kb'):\n",
    "                        # if same annotation continue\n",
    "                        if kb_annotation1 == kb_annotation2:\n",
    "                            continue\n",
    "                        # If empty list create list\n",
    "                        if kb_annotation1.features['key'] not in dictKbToKb:\n",
    "                                dictKbToKb[kb_annotation1.features['key']] = []\n",
    "                        # Add key to list\n",
    "                        dictKbToKb[kb_annotation1.features['key']] = dictKbToKb[\n",
    "                            kb_annotation1.features['key']] +  [kb_annotation2.features['key']]\n",
    "            # For each paragraph\n",
    "            for ann in sentenceann:\n",
    "                # Making the rm links\n",
    "                for kb_annotation in pdoc.annset().within(ann).with_type('kb'):\n",
    "                    for network_annoation in pdoc.annset().within(ann).with_type('network'):\n",
    "                        rm.increaseBy(self.gateExtractor.dict_kb[kb_annotation.features['key']], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "            # Adding transitivity relations\n",
    "            for ann in sentenceann:\n",
    "                # Making the rm links\n",
    "                for network_annoation in pdoc.annset().within(ann).with_type('network'):\n",
    "                    for kb_annotation in pdoc.annset().within(ann).with_type('kb'):\n",
    "                        if kb_annotation.features['key'] not in dictKbToKb:\n",
    "                            continue\n",
    "                        for transitivityKey in dictKbToKb[kb_annotation.features['key']]:\n",
    "                            if rm.getValue(self.gateExtractor.dict_kb[transitivityKey], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']]) is None:\n",
    "                                # If link does not exists, then create one\n",
    "                                rm.increaseBy(self.gateExtractor.dict_kb[transitivityKey], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "            self.gs.del_resource(doc)\n",
    "        return rm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f824bf",
   "metadata": {},
   "source": [
    "## Relationship Discovery - COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c6b16de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "corpusJournal = gs.getCorpus4Name('PreDiViD-Journal-11-19')\n",
    "rd = RelationshipDiscovery(corpusJournal, gateExtractor,gs)\n",
    "rmSentence = rd.rmGen.sentenceTermMatching('PreDiViD-Journal-2019-11-Sentence')\n",
    "df_rmSentence = rmToRelationCSV(rmSentence, 'Journal_COVID', 1, 'hasPresence',cluster_date='2019-11') \n",
    "df_rmSentence.to_csv(path_to_relation_folder+rmSentence.matrix_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d7eba92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 726/726 [01:05<00:00, 11.02it/s]\n"
     ]
    }
   ],
   "source": [
    "corpusMedical = gs.getCorpus4Name('PreDiViD-Medical-12-19')\n",
    "rd = RelationshipDiscovery(corpusMedical, gateExtractor,gs)\n",
    "rmSentence = rd.rmGen.sentenceTermMatching('PreDiViD-Medical-2019-12-Sentence')\n",
    "df_rmSentence = rmToRelationCSV(rmSentence, 'Medical_COVID', 1, 'hasPresence',cluster_date='2019-12') \n",
    "df_rmSentence.to_csv(path_to_relation_folder+rmSentence.matrix_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d938e707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4908/4908 [04:41<00:00, 17.44it/s]\n"
     ]
    }
   ],
   "source": [
    "corpusSocial = gs.getCorpus4Name('PreDiViD-Social-2-20') # CORRECT corpus dates\n",
    "rd = RelationshipDiscovery(corpusSocial, gateExtractor,gs)\n",
    "rmSentence = rd.rmGen.sentenceTermMatching('PreDiViD-Social-2020-20-Sentence')\n",
    "df_rmSentence = rmToRelationCSV(rmSentence, 'Social_COVID', 1, 'hasPresence',cluster_date='2020-02') \n",
    "df_rmSentence.to_csv(path_to_relation_folder+rmSentence.matrix_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f689369",
   "metadata": {},
   "source": [
    "## Relationship Discovery - Monkeypox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69ece3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 27/27 [00:03<00:00,  7.09it/s]\n"
     ]
    }
   ],
   "source": [
    "corpusJournal = gs.getCorpus4Name('PreDiViD-Monkeypox-journal-2022-5')\n",
    "rd = RelationshipDiscovery(corpusJournal, gateExtractor,gs)\n",
    "rmSentence = rd.rmGen.sentenceTermMatching('PreDiViD-Monkeypox-journal-2022-5-Sentence')\n",
    "df_rmSentence = rmToRelationCSV(rmSentence, 'Journal_Monkeypox', 1, 'hasPresence',cluster_date='2022-05') \n",
    "df_rmSentence.to_csv(path_to_relation_folder+rmSentence.matrix_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4499f886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 36/36 [00:02<00:00, 13.80it/s]\n"
     ]
    }
   ],
   "source": [
    "corpusMedical = gs.getCorpus4Name('PreDiViD-Monkeypox-pubmed-2022-6')\n",
    "rd = RelationshipDiscovery(corpusMedical, gateExtractor,gs)\n",
    "rmSentence = rd.rmGen.sentenceTermMatching('PreDiViD-Monkeypox-medical-2022-6-Sentence')\n",
    "df_rmSentence = rmToRelationCSV(rmSentence, 'Medical_Monkeypox', 1, 'hasPresence',cluster_date='2022-06') \n",
    "df_rmSentence.to_csv(path_to_relation_folder+rmSentence.matrix_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12c882dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 33826/33826 [18:45<00:00, 30.05it/s]\n"
     ]
    }
   ],
   "source": [
    "corpusSocial = gs.getCorpus4Name('PreDiViD-Monkeypox-Social-2022-5') # CORRECT corpus dates\n",
    "rd = RelationshipDiscovery(corpusSocial, gateExtractor,gs)\n",
    "rmSentence = rd.rmGen.sentenceTermMatching('PreDiViD-Monkeypox-Social-2022-5-Sentence')\n",
    "df_rmSentence = rmToRelationCSV(rmSentence, 'Social_Monkeypox', 1, 'hasPresence',cluster_date='2022-05') \n",
    "df_rmSentence.to_csv(path_to_relation_folder+rmSentence.matrix_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf748b",
   "metadata": {},
   "source": [
    "## Transitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6afb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gs.getCorpus4Name('PreDiViD')\n",
    "rmGen = RMGenerator(corpus, gateExtractor, gs)\n",
    "rd = RelationshipDiscovery(corpus, gateExtractor,gs,rmGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7991e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmSentence = rd.rmGen.sentenceTermMatchingTransitivity('PreDiViD-COVID-Journal-2019-11-Sentence-Transitivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e619bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmParagraph = rd.rmGen.paragraphTermMatching('PreDiViD-COVID-Journal-2019-11-Paragraph-Transitivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6958da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmParagraph = rmToRelationCSV(rmParagraph, 'Journal', 1, 'hasPresence',cluster_date='2019-11') \n",
    "df_rmParagraph.to_csv(path_to_relation_folder+rmParagraph.matrix_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmSentence = rmToRelationCSV(rmSentence, 'Journal', 1, 'hasPresence',cluster_date='2019-11') \n",
    "df_rmSentence.to_csv(path_to_relation_folder+rmSentence.matrix_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b7ab8",
   "metadata": {},
   "source": [
    "### Observation Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0221bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.kgce.schema.semantic.neo4jclasses import Neo4jRelation\n",
    "from lib.kgce.neo4j.handler import Neo4jWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e4934a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Neo4jWrapper:\n",
    "\n",
    "    def __init__(self, uri, userName, password):\n",
    "        self.uri = uri\n",
    "        self.userName = userName\n",
    "        self.password = password\n",
    "        # Connect to the neo4j database server\n",
    "        self.graphDB_Driver  = GraphDatabase.driver(uri, auth=(userName, password)) \n",
    "        \n",
    "    def sendQuery(self, cql_commands):\n",
    "        result = []\n",
    "        done_queries = []\n",
    "        with self.graphDB_Driver.session() as graphDB_Session:\n",
    "            for cqlCreate in tqdm(cql_commands):\n",
    "                try:\n",
    "                    result += [graphDB_Session.run(cqlCreate).to_df()]\n",
    "                    done_queries.append(cqlCreate)\n",
    "                except Exception as e:\n",
    "                    tqdm.write(str(e))\n",
    "                    tqdm.write(cqlCreate)\n",
    "                    result += [str(e)]\n",
    "        return result\n",
    "    \n",
    "    def closeConnection(self):\n",
    "        self.graphDB_Driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06688534",
   "metadata": {},
   "outputs": [],
   "source": [
    "neowrapper = Neo4jWrapper(uri=\"bolt://localhost:7687\",userName=\"neo4j\",password=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39939a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetObservationFromSource(neowrapper,source, filterValue):\n",
    "    strQuery = \"\"\"MATCH (n:Country)<-[r:hasPresence]-(c) \n",
    "        WHERE toInteger(r.intensity) >= {0} AND r.source = \"{1}\"\n",
    "        RETURN n.wkgs_nameEn as System_Name, n.id, c.name, c.id, r.intensity as intensity;\"\"\".format(\n",
    "        filterValue, source)\n",
    "    result = neowrapper.sendQuery([strQuery])\n",
    "    df_result_journal = result[0].groupby(['System_Name','n.id'],as_index=False).agg(list)\n",
    "    return df_result_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "78e26642",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.74it/s]\n"
     ]
    }
   ],
   "source": [
    "df_observation_journal_covid = GetObservationFromSource(neowrapper,\"Journal_COVID\",5)\n",
    "df_observation_medical_covid = GetObservationFromSource(neowrapper,\"Medical_COVID\",8)\n",
    "df_observation_social_covid = GetObservationFromSource(neowrapper,\"Social_COVID\",10)\n",
    "df_observation_journal_monkey = GetObservationFromSource(neowrapper,\"Journal_Monkeypox\",5)\n",
    "df_observation_medical_monkey = GetObservationFromSource(neowrapper,\"Medical_Monkeypox\",3)\n",
    "df_observation_social_monkey = GetObservationFromSource(neowrapper,\"Social_Monkeypox\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9e5ca32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving COVID\n",
    "df_observation_journal_covid.to_csv(path_to_covid_journalobservationcsv)\n",
    "df_observation_medical_covid.to_csv(path_to_covid_medicalobservationcsv)\n",
    "df_observation_social_covid.to_csv(path_to_covid_socialobservationcsv)\n",
    "# Saving Monkeypox\n",
    "df_observation_journal_monkey.to_csv(path_to_monkeypox_journalobservationcsv)\n",
    "df_observation_medical_monkey.to_csv(path_to_monkeypox_medicalobservationcsv)\n",
    "df_observation_social_monkey.to_csv(path_to_monkeypox_socialobservationcsv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvenv",
   "language": "python",
   "name": "dtvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
