{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d84d91",
   "metadata": {},
   "source": [
    "# Detection Module\n",
    "\n",
    "    The main goal of the detection module is to use the gazetteers out of the ontologies used to enrich PropaPhen into PropaPhen+ to discover relationships between network nodes/systems and the gufo:Entities by text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "110efc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ceac9",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50975ef2",
   "metadata": {},
   "source": [
    "### Installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "c1d1a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install tqdm\n",
    "#!pip install nltk\n",
    "#!pip install gatenlp\n",
    "#!pip install py4j\n",
    "#!pip install pyodide\n",
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d43728",
   "metadata": {},
   "source": [
    "### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3dc96c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438eb90",
   "metadata": {},
   "source": [
    "### Custom libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "7296f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection.relationdiscovery\n",
    "import detection.observationclustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa39dd",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "55a0d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_covid_journals = \"data/textual/covid/newspaper/\"\n",
    "path_to_kb_gazetteer = \"data/gazetteers/kbgazetteer.csv\"\n",
    "path_to_netwoork_gazetteer = \"data/gazetteers/world_gazetteer_en.csv\"\n",
    "path_to_lsts = \"data/lst/\"\n",
    "path_to_relationcsv = \"data/csv/discoveredRelationships.csv\"\n",
    "path_to_observationcsv = \"data/csv/observations.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1179f",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "bc2e23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Term:\n",
    "    \"\"\"A Term is a singleword or a multiword \n",
    "    string that refers to a single unit of knowledge.\n",
    "    They represent the words of interest in the corpus.\n",
    "    \"\"\"\n",
    "    def __init__(self, label : str) -> None:\n",
    "        self.label = label\n",
    "        self.termRepresentation = None\n",
    "        \n",
    "    def termRepresentationFunction(self, representationFunction) -> None:\n",
    "        \"\"\"Updates de termRepresentation variable\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        representationFunction : Function\n",
    "            Function that extracts the representation of the term\n",
    "        \"\"\"\n",
    "        self.termRepresentation = representationFunction(self.label)\n",
    "        \n",
    "    def similarityValue(self,similarityFunction, otherTerm) -> float:\n",
    "        \"\"\"Retrieves the similarity value out of two terms\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        similarityFunction : Function\n",
    "            Function for similarity retrieval\n",
    "        otherTerm : Term\n",
    "            Second term for the similarity function\n",
    "        Returns\n",
    "        ----------\n",
    "        Value of similarity between terms\n",
    "        \"\"\"\n",
    "        assert self.termRepresentation is not None\n",
    "        assert otherTerm.termRepresentation is not None\n",
    "        return similarityFunction(self.termRepresentation,otherTerm.termRepresentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c0843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concept:\n",
    "    \"\"\"It is a conceptualization of an unit of \n",
    "    knowledge that may have multiple Terms associated with.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_of_terms=None,list_of_ids=None):\n",
    "        if list_of_terms is None:\n",
    "            self.list_of_terms = []\n",
    "        else:\n",
    "            self.list_of_terms = list_of_terms\n",
    "        if list_of_ids is None:\n",
    "            self.list_of_ids = []\n",
    "        else:\n",
    "            self.list_of_ids = list_of_ids\n",
    "    \n",
    "    def setOfTermStrings(self,cleaningFunction=None):\n",
    "        \"\"\"Returns a clean list of all term's strings\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaningFunction : Function\n",
    "            Function for normalizing and cleaning every string if necessary\n",
    "        Returns\n",
    "        ----------\n",
    "        Cleanned string list\n",
    "        \"\"\"\n",
    "        termList = list(set([term.label for term in self.list_of_terms]))\n",
    "        if cleaningFunction is not None:\n",
    "            for i in range(len(termList)):\n",
    "                termList[i] = cleaningFunction(termList[i])\n",
    "        return termList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c0a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_concepts(df):\n",
    "    dict_id = {}\n",
    "    dict_concept = {}\n",
    "    print(\"Finding Terms\")\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        if row[\"ID\"] in dict_id:\n",
    "            inDict = False\n",
    "            # Check for duplicatas\n",
    "            for t in dict_id[row[\"ID\"]].list_of_terms:\n",
    "                if row[\"Name\"] == t.label:\n",
    "                    inDict = True\n",
    "                    break\n",
    "            # If no duplicatas\n",
    "            if inDict == False:\n",
    "                newTerm = Term(row[\"Name\"])\n",
    "                dict_id[row[\"ID\"]].list_of_terms.append(newTerm)\n",
    "                dict_concept[row[\"Name\"]] = dict_id[row[\"ID\"]]\n",
    "        elif row[\"Name\"] in dict_concept:\n",
    "            dict_concept[row[\"Name\"]].list_of_ids.append(row[\"ID\"])\n",
    "            dict_id[row[\"ID\"]] = dict_concept[row[\"Name\"]]\n",
    "        else:\n",
    "            newterm = Term(row[\"Name\"])\n",
    "            newconcept = Concept([newterm],[row[\"ID\"]])\n",
    "            dict_concept[row[\"Name\"]] = newconcept\n",
    "            dict_id[row[\"ID\"]] = newconcept\n",
    "    print(\"Creating Term list\")\n",
    "    listset = set()\n",
    "    for key in dict_id:\n",
    "        listset.add(dict_id[key])\n",
    "    for key in dict_concept:\n",
    "        listset.add(dict_concept[key])\n",
    "    return list(listset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c55709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningPlaceStr(string):\n",
    "    return str(string).replace('\"','')\n",
    "\n",
    "def capPlaceStr(string):\n",
    "    return cleaningPlaceStr(string).upper()\n",
    "\n",
    "def lowerPlaceStr(string):\n",
    "    return cleaningPlaceStr(string).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "955ac1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conceptsToGazetteer(concept_list,path_to_list, cleanningFunction=None):\n",
    "    \"\"\"Save the list of concetps into a gazetteer, while it returns the dict{term:concept}\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        concept_list : List of Concepts\n",
    "            List of all concepts that are going to make part of the gazetteer\n",
    "        cleaningFunction : Function\n",
    "            Function for normalizing and cleaning every string if necessary\n",
    "        Returns\n",
    "        ----------\n",
    "        Saves file into path and returns dictionary of terms from the concepts having their equivalent concept as value\n",
    "    \"\"\"\n",
    "    list_str = \"\"\n",
    "    dict_term_concept = {}\n",
    "    for concept in tqdm(concept_list):\n",
    "        for term in concept.setOfTermStrings(cleanningFunction):\n",
    "            list_str += term + \"\\n\"\n",
    "            dict_term_concept[term] = concept\n",
    "    with open(path_to_list, \"w\") as text_file:\n",
    "        text_file.write(list_str)\n",
    "    return dict_term_concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f1ea4",
   "metadata": {},
   "source": [
    "## Relationship Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e82cd",
   "metadata": {},
   "source": [
    "### KB Gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560db393",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_concept_list = []\n",
    "network_concept_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d665080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kb = pd.read_csv(path_to_kb_gazetteer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efb8f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C0026106</td>\n",
       "      <td>Mild mental retardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C0026351</td>\n",
       "      <td>Moderate mental retardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C0036857</td>\n",
       "      <td>Severe mental retardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C0020796</td>\n",
       "      <td>Profound mental retardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C0025362</td>\n",
       "      <td>Unspecified mental retardation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        ID                            Name\n",
       "0           0  C0026106         Mild mental retardation\n",
       "1           1  C0026351     Moderate mental retardation\n",
       "2           2  C0036857       Severe mental retardation\n",
       "3           3  C0020796     Profound mental retardation\n",
       "4           4  C0025362  Unspecified mental retardation"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c71050a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12620098it [12:57, 16235.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Term list\n"
     ]
    }
   ],
   "source": [
    "kb_concept_list = df_to_concepts(df_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8c71501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7892473"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kb_concept_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7fb0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umlsConceptCleanner(concept : Concept):\n",
    "    terms_to_remove = []\n",
    "    terms_to_add = []\n",
    "    for term in concept.list_of_terms:\n",
    "        label = str(term.label)\n",
    "        if len(label) == 0:\n",
    "            terms_to_remove.append(term)\n",
    "        if len(label.split(':')) > 1:\n",
    "            terms_to_remove.append(term)\n",
    "            for l in label.split(':'):\n",
    "                newterm = Term(l)\n",
    "                terms_to_add.append(newterm)\n",
    "    for term in terms_to_remove:\n",
    "        concept.list_of_terms.remove(term)\n",
    "    concept.list_of_terms += terms_to_add\n",
    "    return concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ec155cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    return s.isascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f920910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worldConceptCleanner(concept : Concept):\n",
    "    terms_to_remove = []\n",
    "    terms_to_add = []\n",
    "    for term in concept.list_of_terms:\n",
    "        label = str(term.label)\n",
    "        if not isEnglish(label):\n",
    "            terms_to_remove.append(term)\n",
    "            continue\n",
    "        if len(label.split(':')) > 1:\n",
    "            terms_to_remove.append(term)\n",
    "            for l in label.split(':'):\n",
    "                newterm = Term(l)\n",
    "                terms_to_add.append(newterm)\n",
    "    for term in terms_to_remove:\n",
    "        concept.list_of_terms.remove(term)\n",
    "    concept.list_of_terms += terms_to_add\n",
    "    return concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f22d91e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 7892473/7892473 [00:33<00:00, 234677.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(kb_concept_list))):\n",
    "    kb_concept_list[i] = umlsConceptCleanner(kb_concept_list[i])\n",
    "    kb_concept_list[i] = umlsConceptCleanner(kb_concept_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41aba865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 7892473/7892473 [02:16<00:00, 57729.56it/s]\n"
     ]
    }
   ],
   "source": [
    "umlsdict = conceptsToGazetteer(kb_concept_list,path_to_lsts+\"umls.lst\",cleaningPlaceStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21f141",
   "metadata": {},
   "source": [
    "### Place Gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "04d8c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClearnWorldKGGazetteer(df_network,clear_net_list):\n",
    "    df_network = df_network.drop(df_network[(df_network[\"Name\"]=='\"China\"') & (df_network[\"ID\"]!=\"wkg:424313582\")].index)\n",
    "    df_network = df_network[~df_network['Name'].isin(clear_net_list)]\n",
    "    return df_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "db6bdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network = pd.read_csv(path_to_netwoork_gazetteer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "dc09a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_net_list = ['\"Nga\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "26be0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network = ClearnWorldKGGazetteer(df_network,clear_net_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "787ed220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, ID, Name]\n",
       "Index: []"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_network[df_network['Name'] == '\"Nga\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "6d60d0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wkg:10</td>\n",
       "      <td>\"Mamassita\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wkg:10</td>\n",
       "      <td>\"Mamacita\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wkg:1000709658</td>\n",
       "      <td>\"Boulzazen\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>wkg:1000709658</td>\n",
       "      <td>\"Boulzazen\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wkg:1000709660</td>\n",
       "      <td>\"Tizi El Oued\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              ID            Name\n",
       "0           0          wkg:10     \"Mamassita\"\n",
       "1           1          wkg:10      \"Mamacita\"\n",
       "2           2  wkg:1000709658     \"Boulzazen\"\n",
       "3           3  wkg:1000709658     \"Boulzazen\"\n",
       "4           4  wkg:1000709660  \"Tizi El Oued\""
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_network.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "40e0a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1692476it [03:36, 7830.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Term list\n"
     ]
    }
   ],
   "source": [
    "network_concept_list = df_to_concepts(df_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "ab5772e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948998\n"
     ]
    }
   ],
   "source": [
    "print(len(network_concept_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "5e35554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing network\n",
    "#for i in tqdm(range(len(network_concept_list))):\n",
    "#    network_concept_list[i] = worldConceptCleanner(network_concept_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "63ccbe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usual name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 948998/948998 [00:03<00:00, 306281.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Normal\n",
    "print(\"Usual name\")\n",
    "normalplacesdict = conceptsToGazetteer(network_concept_list,path_to_lsts+\"places.lst\",cleaningPlaceStr)\n",
    "# Cap\n",
    "#print(\"Cap name\")\n",
    "#capdict = conceptsToGazetteer(network_concept_list,path_to_lsts+\"places_cap.lst\",capPlaceStr)\n",
    "# Lower\n",
    "#print(\"Lower name\")\n",
    "#lowerdict = conceptsToGazetteer(network_concept_list,path_to_lsts+\"places_lower.lst\",lowerPlaceStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd1b0e",
   "metadata": {},
   "source": [
    "### GATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12e63834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gatenlp import Document\n",
    "from gatenlp.gateworker import GateWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ade2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GateWorker(start=False, auth_token=\"1234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba2e71",
   "metadata": {},
   "source": [
    "### Gate Gazetteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54c0e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gatenlp import Document\n",
    "from gatenlp.processing.gazetteer import TokenGazetteer, StringGazetteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "450c694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer based on the NLTK WordPunctTokenizer. \n",
    "from gatenlp.processing.tokenizer import NLTKTokenizer\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a391034b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus = gs.getCorpus4Name('PreDiViD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "050aff86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RelationMatrix():\n",
    "    \n",
    "    def __init__(self, matrix_id):\n",
    "        self.matrix_dict = {}\n",
    "        self.matrix_id = matrix_id\n",
    "        \n",
    "    def getValue(self,i,j):\n",
    "        if (i,j) in self.matrix_dict.keys():\n",
    "            return self.matrix_dict[(i,j)]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def setValue(self,i,j,v):\n",
    "        self.matrix_dict[(i,j)] = v\n",
    "        \n",
    "    def increaseBy(self,i,j,v):\n",
    "        v0 = self.getValue(i,j)\n",
    "        if v0 is None:\n",
    "            v0 = 0\n",
    "        self.setValue(i,j,v+v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "11b0db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMGenerator():\n",
    "    \n",
    "    def __init__(self, corpus,gateExtractor):\n",
    "        self.corpus = corpus\n",
    "        self.gateExtractor = gateExtractor\n",
    "    \n",
    "    def directTermMatching(self, matrix_id):\n",
    "        rm = RelationMatrix(matrix_id)\n",
    "        # Per document\n",
    "        for doc in tqdm(self.corpus):\n",
    "            pdoc = gs.gdoc2pdoc(doc)\n",
    "            pdoc = self.gateExtractor.tokenizer(pdoc)\n",
    "            pdoc = self.gateExtractor.tok_gaz(pdoc)\n",
    "            # Making the rm links\n",
    "            for kb_annotation in pdoc.annset().with_type(\"kb\"):\n",
    "                for network_annoation in pdoc.annset().with_type(\"network\"):\n",
    "                    rm.increaseBy(self.gateExtractor.dict_kb[kb_annotation.features['key']], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "            gs.del_resource(doc)\n",
    "        return rm\n",
    "    \n",
    "    def paragraphTermMatching(self, matrix_id):\n",
    "        assert 'annie' in self.gateExtractor.extra_pr.keys()\n",
    "        rm = RelationMatrix(matrix_id)\n",
    "        # Per document\n",
    "        for doc in tqdm(self.corpus):\n",
    "            # Run annie\n",
    "            gs.worker.run4Document(self.gateExtractor.extra_pr['annie'], doc)\n",
    "            pdoc = gs.gdoc2pdoc(doc)            \n",
    "            # Get network and kb\n",
    "            pdoc = self.gateExtractor.tok_gaz(pdoc)\n",
    "            # Get paragraph\n",
    "            praragraphann = pdoc1.annset('Original markups').with_type(\"paragraph\")\n",
    "            # For each paragraph\n",
    "            for ann in praragraphann:\n",
    "                # Making the rm links\n",
    "                for kb_annotation in pdoc.annset().within(ann).with_type('kb'):\n",
    "                    for network_annoation in pdoc.annset().within(ann).with_type('network'):\n",
    "                        rm.increaseBy(self.gateExtractor.dict_kb[kb_annotation.features['key']], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "            gs.del_resource(doc)\n",
    "        return rm\n",
    "    \n",
    "    def sentenceTermMatching(self, matrix_id):\n",
    "        assert 'annie' in self.gateExtractor.extra_pr.keys()\n",
    "        rm = RelationMatrix(matrix_id)\n",
    "        # Per document\n",
    "        for doc in tqdm(self.corpus):\n",
    "            # Run annie\n",
    "            gs.worker.run4Document(self.gateExtractor.extra_pr['annie'], doc)\n",
    "            pdoc = gs.gdoc2pdoc(doc)            \n",
    "            # Get network and kb\n",
    "            pdoc = self.gateExtractor.tok_gaz(pdoc)\n",
    "            # Get paragraph\n",
    "            sentenceann = pdoc1.annset('').with_type(\"Sentence\")\n",
    "            # For each paragraph\n",
    "            for ann in sentenceann:\n",
    "                # Making the rm links\n",
    "                for kb_annotation in pdoc.annset().within(ann).with_type('kb'):\n",
    "                    for network_annoation in pdoc.annset().within(ann).with_type('network'):\n",
    "                        rm.increaseBy(self.gateExtractor.dict_kb[kb_annotation.features['key']], \n",
    "                                self.gateExtractor.dict_network[network_annoation.features['key']],1)\n",
    "            gs.del_resource(doc)\n",
    "        return rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96e0b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationshipDiscovery():\n",
    "    \n",
    "    def __init__(self,corpus, gateExtractor, rmGen=None):\n",
    "        self.corpus = corpus\n",
    "        self.gateExtractor = gateExtractor\n",
    "        if rmGen is not None:\n",
    "            self.rmGen = rmGen\n",
    "            assert self.rmGen.corpus == self.corpus\n",
    "            assert self.rmGen.gateExtractor == self.gateExtractor\n",
    "        else:\n",
    "            self.rmGen = RMGenerator(self.corpus, self.gateExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "f93a4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateExtractor():\n",
    "    \n",
    "    def __init__(self, dict_kb, dict_network, extra_pr=None):\n",
    "        self.tokenizer = NLTKTokenizer(\n",
    "            nltk_tokenizer=WordPunctTokenizer(), \n",
    "            token_type=\"Token\", outset_name=\"\")\n",
    "        self.dict_kb = dict_kb\n",
    "        self.dict_network = dict_network\n",
    "        print('Creating KB gazetteer...')\n",
    "        self.kb_gazetteer = self.gazetteer_creator(self.dict_kb.keys())\n",
    "        print('Creating Network gazetteer...')\n",
    "        self.network_gazetteer = self.gazetteer_creator(self.dict_network.keys())\n",
    "        print('Creating Merging gazetteer...')\n",
    "        self.tok_gaz = TokenGazetteer(longest_only=False,\n",
    "                          skip_longest=False, outset_name=\"\", ann_type=\"Lookup\",\n",
    "                          annset_name=\"\", token_type=\"Token\")\n",
    "        self.tok_gaz.append(source=self.kb_gazetteer, source_fmt=\"gazlist\", list_type=\"kb\")\n",
    "        self.tok_gaz.append(source=self.network_gazetteer, source_fmt=\"gazlist\", list_type=\"network\")\n",
    "        if extra_pr is None:\n",
    "            self.extra_pr = {}\n",
    "        else:\n",
    "            self.extra_pr = extra_pr\n",
    "        self.extra_pr['tokenizer'] = self.tokenizer\n",
    "        self.extra_pr['tok_gaz'] = self.tok_gaz\n",
    "        \n",
    "    def _text2tokenstrings(self, text):\n",
    "        tmpdoc = Document(text)\n",
    "        self.tokenizer(tmpdoc)\n",
    "        tokens = list(tmpdoc.annset().with_type(\"Token\"))\n",
    "        return [tmpdoc[tok] for tok in tokens]\n",
    "    \n",
    "    def gazetteer_creator(self, list_of_entries):\n",
    "        return [(self._text2tokenstrings(txt),\n",
    "                            {'key' : txt}) for txt in tqdm(list_of_entries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a7c893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67a719bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string library function  \n",
    "import string  \n",
    "    \n",
    "# Storing the sets of punctuation in variable result  \n",
    "punctuation = [i for i in string.punctuation  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47314425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanKeys(dictionary, clean_list):\n",
    "    for c in clean_list:\n",
    "        if c in dictionary:\n",
    "            del dictionary[c]\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "b3250c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = stopwords_list + punctuation\n",
    "normalplacesdict = cleanKeys(normalplacesdict,clean_list)\n",
    "umlsdict = cleanKeys(umlsdict,clean_list+list(normalplacesdict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "7b82c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KB gazetteer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 7657627/7657627 [17:03<00:00, 7484.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Network gazetteer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1217468/1217468 [01:11<00:00, 17025.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Merging gazetteer...\n"
     ]
    }
   ],
   "source": [
    "gateExtractor = GateExtractor(umlsdict,normalplacesdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "c29edd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANNIE'"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Annie\n",
    "gs.worker.loadMavenPlugin(\"uk.ac.gate.plugins\", \"annie\", \"8.6\")\n",
    "# now load the prepared ANNIE pipeline from the plugin\n",
    "pipeline = gs.worker.loadPipelineFromPlugin(\"uk.ac.gate.plugins\",\"annie\", \"/resources/ANNIE_with_defaults.gapp\")\n",
    "pipeline.getName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "3e739724",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateExtractor.extra_pr['annie'] = pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "c6b16de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = RelationshipDiscovery(corpus, gateExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "b83d2e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 8/8 [00:25<00:00,  3.13s/it]\n"
     ]
    }
   ],
   "source": [
    "rm = rd.rmGen.directTermMatching('PreDiViD-11-2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "4ff4ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.80it/s]\n"
     ]
    }
   ],
   "source": [
    "rm = rd.rmGen.paragraphTermMatching('PreDiViD-11-2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "5ba4aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "rm = rd.rmGen.sentenceTermMatching('PreDiViD-11-2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3e2ac",
   "metadata": {},
   "source": [
    "### Neo4J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b28fdb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmToRelationCSV(rm, source_value, trustworthiness_value, typeof_value, cluster_date=None):\n",
    "    list_start = []\n",
    "    list_end = []\n",
    "    date = []\n",
    "    source = []\n",
    "    trustworthiness = []\n",
    "    typeof = []\n",
    "    intensity = []\n",
    "    if cluster_date is None:\n",
    "        cluster_date = '-'.join(rm.matrix_id.split('-')[1:])\n",
    "    for key in list(rm.matrix_dict.keys()):\n",
    "        c1 = key[0]\n",
    "        c2 = key[1]\n",
    "        intentisyValue = rm.matrix_dict[key]\n",
    "        for c1id in c1.list_of_ids:\n",
    "            for c2id in c2.list_of_ids:\n",
    "                list_start.append(c1id)\n",
    "                list_end.append(c2id)\n",
    "                intensity.append(intentisyValue)\n",
    "    date = [cluster_date] * len(list_start)\n",
    "    source = [source_value] * len(list_start)\n",
    "    trustworthiness = [trustworthiness_value] * len(list_start)\n",
    "    typeof = [typeof_value] * len(list_start)\n",
    "    df = pd.DataFrame(data={\n",
    "        \":START_ID\" : list_start,\n",
    "        \":END_ID\" : list_end,\n",
    "        \":TYPE\" : typeof,\n",
    "        \"date\" : date,\n",
    "        \"source\" : source,\n",
    "        \"trustworthiness\" : trustworthiness,\n",
    "        \"intensity\" : intensity\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "46b969a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rm = rmToRelationCSV(rm, 'Journal', 1, 'hasPresence') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "9934387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rm.to_csv(path_to_relationcsv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b7ab8",
   "metadata": {},
   "source": [
    "### Observation Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0221bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.kgce.schema.semantic.neo4jclasses import Neo4jRelation\n",
    "from lib.kgce.neo4j.handler import Neo4jWrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5e4934a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Neo4jWrapper:\n",
    "\n",
    "    def __init__(self, uri, userName, password):\n",
    "        self.uri = uri\n",
    "        self.userName = userName\n",
    "        self.password = password\n",
    "        # Connect to the neo4j database server\n",
    "        self.graphDB_Driver  = GraphDatabase.driver(uri, auth=(userName, password)) \n",
    "        \n",
    "    def sendQuery(self, cql_commands):\n",
    "        result = []\n",
    "        done_queries = []\n",
    "        with self.graphDB_Driver.session() as graphDB_Session:\n",
    "            for cqlCreate in tqdm(cql_commands):\n",
    "                try:\n",
    "                    result += [graphDB_Session.run(cqlCreate).to_df()]\n",
    "                    done_queries.append(cqlCreate)\n",
    "                except Exception as e:\n",
    "                    tqdm.write(str(e))\n",
    "                    tqdm.write(cqlCreate)\n",
    "                    result += [str(e)]\n",
    "        return result\n",
    "    \n",
    "    def closeConnection(self):\n",
    "        self.graphDB_Driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "06688534",
   "metadata": {},
   "outputs": [],
   "source": [
    "neowrapper = Neo4jWrapper(uri=\"bolt://localhost:7687\",userName=\"neo4j\",password=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "a36e5639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.41it/s]\n"
     ]
    }
   ],
   "source": [
    "result = neowrapper.sendQuery([\n",
    "    \"\"\"MATCH (n:Country)<-[r:hasPresence]-(c) \n",
    "    WHERE toInteger(r.intensity) >= 1\n",
    "    RETURN n.wkgs_nameEn as System_Name, n.id, c.name, c.id, r.intensity as intensity;\"\"\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "963862da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = result[0].groupby(['System_Name','n.id'],as_index=False).agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "29042d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System_Name</th>\n",
       "      <th>n.id</th>\n",
       "      <th>c.name</th>\n",
       "      <th>c.id</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"China\"</td>\n",
       "      <td>wkg:424313582</td>\n",
       "      <td>[Province, Province, Province]</td>\n",
       "      <td>[C1514578, A7659903, A7850354]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"France\"</td>\n",
       "      <td>wkg:1363947712</td>\n",
       "      <td>[S, E, S, S, K, K, A, K, E, E, Additional, res...</td>\n",
       "      <td>[A32853699, A3122833, A20945140, A3196982, A29...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Germany\"</td>\n",
       "      <td>wkg:1683325355</td>\n",
       "      <td>[research, activity, ulcer Braden scale: activ...</td>\n",
       "      <td>[A19722860, A18564727, C2171311, A19723452, A0...</td>\n",
       "      <td>[4, 2, 2, 4, 2, 2, 2, 2, 4, 2, 8, 4, 4, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"India\"</td>\n",
       "      <td>wkg:424314145</td>\n",
       "      <td>[Study, Viruses, Study, Presence of, Presence ...</td>\n",
       "      <td>[A18323259, A0132816, A16461341, A4791697, A30...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Kazakhstan\"</td>\n",
       "      <td>wkg:424311521</td>\n",
       "      <td>[PG, p, 31, G, f, DN, Filed, RS, OS, URL, PG, ...</td>\n",
       "      <td>[A26709416, A23010387, A20944698, A15569347, A...</td>\n",
       "      <td>[4, 4, 8, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Netherlands\"</td>\n",
       "      <td>wkg:424297217</td>\n",
       "      <td>[various, Medicine, various, kidney, news, kid...</td>\n",
       "      <td>[C3540765, A10759069, A22723323, A29398775, A1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"Portugal\"</td>\n",
       "      <td>wkg:2377028247</td>\n",
       "      <td>[31, RSV, Parainfluenza virus 1, Parainfluenza...</td>\n",
       "      <td>[A34719971, A21144343, A29378504, A32816437, C...</td>\n",
       "      <td>[6, 6, 3, 3, 6, 3, 3, 6, 3, 3, 6, 6, 6, 12, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Russia\"</td>\n",
       "      <td>wkg:424314830</td>\n",
       "      <td>[RS, PG, URL, u, 31, DN, G, 31, RS, PG, HLA-C*...</td>\n",
       "      <td>[A24583098, A32660236, C1710546, A12814951, A2...</td>\n",
       "      <td>[37, 37, 37, 37, 74, 74, 37, 74, 37, 37, 37, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"United States\"</td>\n",
       "      <td>wkg:424317935</td>\n",
       "      <td>[Researchers, European, Department, D, related...</td>\n",
       "      <td>[A26601571, A32734221, A10825217, A12798025, A...</td>\n",
       "      <td>[1, 120, 1, 1, 60, 1, 6, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       System_Name            n.id  \\\n",
       "0          \"China\"   wkg:424313582   \n",
       "1         \"France\"  wkg:1363947712   \n",
       "2        \"Germany\"  wkg:1683325355   \n",
       "3          \"India\"   wkg:424314145   \n",
       "4     \"Kazakhstan\"   wkg:424311521   \n",
       "5    \"Netherlands\"   wkg:424297217   \n",
       "6       \"Portugal\"  wkg:2377028247   \n",
       "7         \"Russia\"   wkg:424314830   \n",
       "8  \"United States\"   wkg:424317935   \n",
       "\n",
       "                                              c.name  \\\n",
       "0                     [Province, Province, Province]   \n",
       "1  [S, E, S, S, K, K, A, K, E, E, Additional, res...   \n",
       "2  [research, activity, ulcer Braden scale: activ...   \n",
       "3  [Study, Viruses, Study, Presence of, Presence ...   \n",
       "4  [PG, p, 31, G, f, DN, Filed, RS, OS, URL, PG, ...   \n",
       "5  [various, Medicine, various, kidney, news, kid...   \n",
       "6  [31, RSV, Parainfluenza virus 1, Parainfluenza...   \n",
       "7  [RS, PG, URL, u, 31, DN, G, 31, RS, PG, HLA-C*...   \n",
       "8  [Researchers, European, Department, D, related...   \n",
       "\n",
       "                                                c.id  \\\n",
       "0                     [C1514578, A7659903, A7850354]   \n",
       "1  [A32853699, A3122833, A20945140, A3196982, A29...   \n",
       "2  [A19722860, A18564727, C2171311, A19723452, A0...   \n",
       "3  [A18323259, A0132816, A16461341, A4791697, A30...   \n",
       "4  [A26709416, A23010387, A20944698, A15569347, A...   \n",
       "5  [C3540765, A10759069, A22723323, A29398775, A1...   \n",
       "6  [A34719971, A21144343, A29378504, A32816437, C...   \n",
       "7  [A24583098, A32660236, C1710546, A12814951, A2...   \n",
       "8  [A26601571, A32734221, A10825217, A12798025, A...   \n",
       "\n",
       "                                           intensity  \n",
       "0                                          [1, 1, 1]  \n",
       "1  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, ...  \n",
       "2      [4, 2, 2, 4, 2, 2, 2, 2, 4, 2, 8, 4, 4, 3, 2]  \n",
       "3  [2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "4  [4, 4, 8, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 8, ...  \n",
       "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "6  [6, 6, 3, 3, 6, 3, 3, 6, 3, 3, 6, 6, 6, 12, 3,...  \n",
       "7  [37, 37, 37, 37, 74, 74, 37, 74, 37, 37, 37, 3...  \n",
       "8  [1, 120, 1, 1, 60, 1, 6, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "b1fe2e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(\"data/csv/observations_phrase.csv\", index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvenv",
   "language": "python",
   "name": "dtvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
